# Amazon EKS Best Practices Guide:

## Security: There are several security best practice areas that are pertinent when using a managed Kubernetes service like EKS

* Identity and Access Management
* Pod Security
* Runtime Security
* Network Security
* Multi-tenancy
* Detective Controls
* Infrastructure Security
* Data Encryption and Secrets Management
* Regulatory Compliance
* Incident Response and Forensics
* Image Security

### Identity and Access Management

It performs two essential function: Authentication and Authorization. Authentication involves verification of identity whereas authorization governs the actions that can be performed by AWS resources

Kubernetes supports different strategy to authenticate request to kube-apiservice e.g Bearer Token, X.509 certificates, OIDC etc

Webhook Authentication strategy calls a webhook that verifies bearer tokens. On EKS these bearer tokens are generated by using AWS CLI or the aws-iam-authenticator client when you run kubectl command. As we execute command, the token is passed to kube apiserver which forwards it to the authentication webhook. If request is well-formed, the webhook calls a pre-signed URL embedded in tokens body. The URL validates the request signature and returns information about the user, eg user account, Arn, and user id to kube apiserver

To manually generate a authentication token, type the following command in a terminal window:
aws eks get-token --cluster-name <cluster_name>

To get a token programmatically:

package main

import (
    "fmt"
    "log"
    "sigs.k8s.io/aws-iam-authenticator/pkg/token"
)

func main()  {
    g, _ := token.NewGenerator(false, false)
    tk, err := g.Get("<cluster_name>")
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(tk)
}

Each token starts with k8s-aws-v1. followed by a base64 encoded string. The token has time to live (TTL) of 15 minutes after which new token need to be generated. Once the user identity has been authenticated by the AWS IAM service, the kube-apiserver reads the aws-auth ConfigMap in the kube-system. Namespace to determine the RBAC group to associate with the user. The aws-auth ConfigMap is used to create a static mapping between IAM principals, i.e. IAM users and roles, and kubernetes RBAC groups.RBAC groups can be referenced in Kubernetes RoleBindings or ClusterRoleBindings

#### Recommendations

1. Don't use service account for authentication: A service account token is long lived, static credential which doesn't expire. At times, you may need to grant an exception for applications that have to consume Kubernetes API from outside cluster, e.g. a CI/CD pipeline application. If such application runs on AWS Infrastructure, like EC2 Instance, consider using an instance profile and mapping that to kubernetes RBAC role in the aws-auth ConfigMap instead.

2. Employ least privileged access to AWS resources: An IAM user does not need to be assigned privileges to AWS resources to access Kubernetes API.  If you need to grant an IAM user access to an EKS cluster, create an entry in the aws-auth ConfigMap for that user that maps to a specific Kubernetes RBAC group.

3. Use IAM Roles when multiple users need identical access to the cluster: Rather than creating an entry for each individual IAM User in the aws-auth ConfigMap, allow those users to assume an IAM Role and map that role to a Kubernetes RBAC group. This will be easier to maintain, especially as the number of users that require access grows.

When assigning K8s RBAC permissions to an IAM role using mapRoles in aws-auth ConfigMap, you should include {{SessionName}} in your username. That way, the audit log will record the session name so you can track who the actual user assume this role along with the CloudTrail log.

- rolearn: arn:aws:iam::XXXXXXXXXXXX:role/testRole
  username: testRole:{{SessionName}}
  groups:
    - system:masters

4. Make the EKS Cluster Endpoint private: By default when you provision an EKS cluster, the API cluster endpoint is set to public, i.e. it can be accessed from the Internet. Despite being accessible from the Internet, the endpoint is still considered secure because it requires all API requests to be authenticated by IAM and then authorized by Kubernetes RBAC

* Configure the EKS cluster endpoint to be private. 

* Leave the cluster endpoint public and specify which CIDR blocks can communicate with the cluster endpoint. The blocks are effectively a whitelisted set of public IP addresses that are allowed to access the cluster endpoint.

* Configure public access with a set of whitelisted CIDR blocks and set private endpoint access to enabled. This will allow public access from a specific range of public IPs while forcing all network traffic between the kubelets (workers) and the Kubernetes API through the cross-account ENIs that get provisioned into the cluster VPC when the control plane is provisioned.

* Create the cluster with a dedicated IAM role: When you create an Amazon EKS cluster, the IAM entity user or role, such as a federated user that creates the cluster, is automatically granted system:masters permissions in the cluster's RBAC configuration. This access cannot be removed and is not managed through the aws-auth ConfigMap

### Pod Security

OPA

### Multi-tenancy

Kubernetes Constructs: In each of these instances following constructs are used to isolate tenants from each other

#### Namespaces
Namespaces are fundamental to implementing soft multi-tenancy. They allow to divide the cluster in logical partitions. Quotas, network policies, service accounts, and other objects needed to implement multi-tenancy are scoped to a namespace

#### Network policies
By default, all the pods in kubernetes cluster are allowed to communicate with each other. This behaviour can be altered using network policies.

Network policies restricts communication between pods using labels or IP address ranges. In a multi-tenant environment where strict network isolation between tenants is required.

Recommend: Deny default rules communication between pods. and, deny all pods to query the dns server for name resolution.

#### Role Based Access Control (RBAC)

Roles and role bindings are the kubernetes objects used to enforce role based access control. Roles contain the list of actions that can be performed against object in cluster. Role bindings specify the individuals or groups to whom the roles apply.

#### Quotas

Quotas a used to define limit on workload hosted in cluster. With Quotas we can specify maximum amount of cpu and memory that pods can consume, or limit the resource that can be allocated in cluster or namespace. Limit ranges allow to declare maximum, minimum and default values of each limit

Overcommiting the resource on shared cluster is often beneficial because it allows maximize the resources. However unbounded access to cluster can cause resource starvation, which can lead to performance degradation and loss of application availability. If pod's request are set too low and actual resource exceeds capacity of node, the node will begin to experience cpu and memory pressure. When this happens pods may be restarted and evicted from the node.

To prevent this from happening, we should plan to impose quotas on namespaces in a multi tenant environment to force tenants to specify requests and limits when scheduling pods on cluster. It will also mitigate a potential denial of service by constraining the amount of resources a pod can consume.

#### Pod Priority and preemption

Pod Priority and preemption can be useful when we want to provide more importance to a pod relative to other pods.  For example, with pod priority you can configure pods from customer A to run at a higher priority than customer B. When there's insufficient capacity available, the scheduler will evict the lower-priority pods from customer B to accommodate the higher-priority pods from customer A. This can be especially handy in a SaaS environment where customers willing to pay a premium receive a higher priority.


### Detective controls

Auditing and Logging - Collecting and analyzing audit logs is useful. Logs can help root cause analysis and attribution. When enough logs have been collected, they can be used to detect anomalous behaviour too. On EKS, the audit logs are sent to Amazon Cloudwatch logs. 

apiVersion: audit.k8s.io/v1beta1
kind: Policy
rules:
  # Log aws-auth configmap changes
  - level: RequestResponse
    namespaces: ["kube-system"]
    verbs: ["update", "patch", "delete"]
    resources:
      - group: "" # core
        resources: ["configmaps"]
        resourceNames: ["aws-auth"]
    omitStages:
      - "RequestReceived"
  - level: None
    users: ["system:kube-proxy"]
    verbs: ["watch"]
    resources:
      - group: "" # core
        resources: ["endpoints", "services", "services/status"]
  - level: None
    users: ["kubelet"] # legacy kubelet identity
    verbs: ["get"]
    resources:
      - group: "" # core
        resources: ["nodes", "nodes/status"]
  - level: None
    userGroups: ["system:nodes"]
    verbs: ["get"]
    resources:
      - group: "" # core
        resources: ["nodes", "nodes/status"]
  - level: None
    users:
      - system:kube-controller-manager
      - system:kube-scheduler
      - system:serviceaccount:kube-system:endpoint-controller
    verbs: ["get", "update"]
    namespaces: ["kube-system"]
    resources:
      - group: "" # core
        resources: ["endpoints"]
  - level: None
    users: ["system:apiserver"]
    verbs: ["get"]
    resources:
      - group: "" # core
        resources: ["namespaces", "namespaces/status", "namespaces/finalize"]
  - level: None
    users:
      - system:kube-controller-manager
    verbs: ["get", "list"]
    resources:
      - group: "metrics.k8s.io"
  - level: None
    nonResourceURLs:
      - /healthz*
      - /version
      - /swagger*
  - level: None
    resources:
      - group: "" # core
        resources: ["events"]
  - level: Request
    users: ["kubelet", "system:node-problem-detector", "system:serviceaccount:kube-system:node-problem-detector"]
    verbs: ["update","patch"]
    resources:
      - group: "" # core
        resources: ["nodes/status", "pods/status"]
    omitStages:
      - "RequestReceived"
  - level: Request
    userGroups: ["system:nodes"]
    verbs: ["update","patch"]
    resources:
      - group: "" # core
        resources: ["nodes/status", "pods/status"]
    omitStages:
      - "RequestReceived"
  - level: Request
    users: ["system:serviceaccount:kube-system:namespace-controller"]
    verbs: ["deletecollection"]
    omitStages:
      - "RequestReceived"
  # Secrets, ConfigMaps, and TokenReviews can contain sensitive & binary data,
  # so only log at the Metadata level.
  - level: Metadata
    resources:
      - group: "" # core
        resources: ["secrets", "configmaps"]
      - group: authentication.k8s.io
        resources: ["tokenreviews"]
    omitStages:
      - "RequestReceived"
  - level: Request
    resources:
      - group: ""
        resources: ["serviceaccounts/token"]
  - level: Request
    verbs: ["get", "list", "watch"]
    resources: 
      - group: "" # core
      - group: "admissionregistration.k8s.io"
      - group: "apiextensions.k8s.io"
      - group: "apiregistration.k8s.io"
      - group: "apps"
      - group: "authentication.k8s.io"
      - group: "authorization.k8s.io"
      - group: "autoscaling"
      - group: "batch"
      - group: "certificates.k8s.io"
      - group: "extensions"
      - group: "metrics.k8s.io"
      - group: "networking.k8s.io"
      - group: "policy"
      - group: "rbac.authorization.k8s.io"
      - group: "scheduling.k8s.io"
      - group: "settings.k8s.io"
      - group: "storage.k8s.io"
    omitStages:
      - "RequestReceived"
  # Default level for known APIs
  - level: RequestResponse
    resources: 
      - group: "" # core
      - group: "admissionregistration.k8s.io"
      - group: "apiextensions.k8s.io"
      - group: "apiregistration.k8s.io"
      - group: "apps"
      - group: "authentication.k8s.io"
      - group: "authorization.k8s.io"
      - group: "autoscaling"
      - group: "batch"
      - group: "certificates.k8s.io"
      - group: "extensions"
      - group: "metrics.k8s.io"
      - group: "networking.k8s.io"
      - group: "policy"
      - group: "rbac.authorization.k8s.io"
      - group: "scheduling.k8s.io"
      - group: "settings.k8s.io"
      - group: "storage.k8s.io"
    omitStages:
      - "RequestReceived"
  # Default level for all other requests.
  - level: Metadata
    omitStages:
      - "RequestReceived"

Recommendations:

1. Enable audit logs
2. Utilize audit metadata
3. Create alarm for suspicious events
4. Analyze logs with log insights

### Network Security

It involves the application of rules which restricts the flow of network traffic between services and encryption of traffic while it is in transit.

Traffic Control:
1. Network Policies 
2. Security Group

Encryption in transit:
1. Service Mesh
2. Container network interfaces
3. Ingress controllers and Load balancers
4. Nitro Instances
5. ACM private CA with cert-manager

#### Network Policies

Secure network traffic in Kubernetes clusters.When a network policy with a policyType Ingress is specified, only allowed connections into the pod are those from the pod's node and those allowed by the ingress rules. Same applies for egress rules. If multiple rules are defined, then union of all rules are taken into account when making the decision. Thus, order of evaluation does not affect the policy result.

Recommendations:

1. Create a default deny policy

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny
  namespace: default
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress

2. Create a rule to allow DNS queries - Once you have the default deny all rule in place, you can begin layering on additional rules, such as a rule that allows pods to query CoreDNS for name resolution.

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-dns-access
  namespace: default
spec:
  podSelector:
    matchLabels: {}
  policyTypes:
  - Egress
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: kube-system
      podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - protocol: UDP
      port: 53

3. Incrementally add rules to selectively allow the flow of traffic between namespaces/pods

Understand the application requirements and create fine-grained ingress and egress rules as needed. Below example shows how to restrict ingress traffic on port 80 to app-one from client-one. This helps minimize the attack surface and reduces the risk of unauthorized access.

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-ingress-app-one
  namespace: default
spec:
  podSelector:
    matchLabels:
      k8s-app: app-one
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          k8s-app: client-one
    ports:
    - protocol: TCP
      port: 80

#### Security Groups

EKS uses AWS VPC Security Groups (SGs) to control the traffic between the Kubernetes control plane and the cluster's worker nodes. Security groups are also used to control the traffic between worker nodes, and other VPC resources, and external IP addresses. 

When to use Network Policy for Pods?

1. Controlling pod-to-pod traffic
2. Control traffic at the IP address or port level (OSI layer 3 or 4)

When to use AWS Security groups for pods (SGP):

1. Leverage existing AWS configurations: If you already have complex set of EC2 security groups that manage access to AWS services
2. Control access to AWS services: Your applications running within an EKS cluster wants to communicate with other AWS services
3. Isolation of Pod & Node traffic: If you want to completely separate pod traffic from the rest of the node traffic, use SGP in POD_SECURITY_GROUP_ENFORCING_MODE=strict mode.

### Data encryption and secrets management

Encryption at rest: There are 3 different AWS native storage option we can use with kubernetes: EBS, EFS and FSX for Lusture. All 3 offers encryption at rest using service managed key or customer master key (CMK)

apiVersion: v1
kind: PersistentVolume
metadata:
  name: efs-pv
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: efs-sc
  mountOptions:
    - tls
  csi:
    driver: efs.csi.aws.com
    volumeHandle: <file_system_id>


The FSx CSI driver supports dynamic provisioning of Lustre file systems. It encrypts data with a service managed key by default, although there is an option to provide your own CMK as in this example:

kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: fsx-sc
provisioner: fsx.csi.aws.com
parameters:
  subnetId: subnet-056da83524edbe641
  securityGroupIds: sg-086f61ea73388fb6b
  deploymentType: PERSISTENT_1
  kmsKeyId: <kms_arn>

Recommendations:

1. Encrypt data at rest - Encrypting data at rest is considered a best practice. If you're unsure whether encryption is necessary, encrypt your data.

2. Rotate your CMKs periodically - Configure KMS to automatically rotate your CMKs. This will rotate your keys once a year while saving old keys indefinitely so that your data can still be decrypted

3. Use EFS access points to simplify access to shared datasets: If you have shared datasets with different POSIX file permissions or want to restrict access to part of the shared file system by creating different mount points, consider using EFS access points

### Runtime Security

Runtime security provides active protection for your containers while they're running. The idea is to detect and/or prevent malicious activity from occurring inside the container. With secure computing (seccomp) you can prevent a containerized application from making certain syscalls to the underlying host operating system's kernel

To get started with seccomp, use strace to generate a stack trace to see which system calls your application is making, then use a tool such as syscall2seccomp to create a seccomp profile from the data gathered from the trace

### Image Security

You should consider the container image as your first line of defense against an attack. An insecure, poorly constructed image can allow an attacker to escape the bounds of the container and gain access to the host. Once on the host, an attacker can gain access to sensitive information or move laterally within the cluster or with your AWS account.

Recommendations:

1. Create minimal images - Remove all extraneous binaries from the container image. 

Remove all binaries with the SETUID and SETGID bits as they can be used to escalate privilege and consider removing all shells and utilities like nc and curl that can be used for nefarious purposes. You can find the files with SETUID and SETGID bits with the following command:

find / -perm /6000 -type f -exec ls -ld {} \;

To remove the special permissions from these files, add the following directive to your container image:

RUN find / -xdev -perm /6000 -type f -exec chmod a-s {} \; || true

2. Use multi-stage builds - Using multi-stage builds is a way to create minimal images. Oftentimes, multi-stage builds are used to automate parts of the Continuous Integration cycle.

3. Scan images for vulnerabilities regularly - Like their virtual machine counterparts, container images can contain binaries and application libraries with vulnerabilities or develop vulnerabilities over time.


## Cluster Autoscaling

### Karpenter

It is an open source cluster autoscaler that automatically provisions new nodes in response to unschedulable pods. Karpenter evaluates the aggregate resource requirement of the pending pods and choose the optimal instance to run them. It will automatically scale in and terminate instance that don't have any non-daemonset pods to reduce waste. It also supports a consolidation feature which will actively move pods around and either delete or replace nodes with cheaper versions to reduce cluster cost.

Reasons to use Karpenter:
Karpenter consolidates instance orchestration responsibilities within a single system, which is simpler, more stable and cluster-aware. Karpenter was designed to overcome some of the challenges presented by Cluster Autoscaler by providing simplified ways to:

1. Provision nodes based on workload requirements.
2. Create diverse node configurations by instance type, using flexible workload provisioner options. Instead of managing many specific custom node groups, Karpenter could let you manage diverse workload capacity with a single, flexible provisioner.
3. Achieve improved pod scheduling at scale by quickly launching nodes and scheduling pods.

# Provisioner for GPU Instances with Taints
apiVersion: karpenter.sh/v1alpha5
kind: Provisioner
metadata:
  name: gpu
spec:
  requirements:
  - key: node.kubernetes.io/instance-type
    operator: In
    values:
    - p3.8xlarge
    - p3.16xlarge
  taints:
  - effect: NoSchedule
    key: nvidia.com/gpu
    value: "true"
  ttlSecondsAfterEmpty: 60






